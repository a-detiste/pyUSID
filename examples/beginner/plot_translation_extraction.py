"""
======================================================================================
03.b Translating from Proprietary File Formats
======================================================================================

**Suhas Somnath**

8/8/2017

This document illustrates an example of extracting data out of proprietary raw data files and writing the information
into a **Universal Spectroscopy and Imaging Data (USID)** HDF5 file (referred to as a **h5USID** file) using tools in
`pyUSID`

Introduction
------------

Before any data analysis, we need to access data stored in the raw file(s) generated by the microscope. Often, the
data and parameters in these files are **not** straightforward to access. In certain cases, additional / dedicated
software packages are necessary to access the data while in many other cases, it is possible to extract the necessary
information from built-in **numpy** or similar python packages included with **anaconda**.

The USID model aims to make data access, storage, curation, etc. simply by storing the data along with all
relevant parameters in a single file (HDF5 for now).

The process of copying data and metadata from the original, proprietary format to **h5USID** files is called
**Translation**. The python classes available in pyUSID and pycroscopy that perform these operation are called
**Translators**.

The rough process of translation is the same regardless of the origin, complexity, or size of the raw data:

1. Investigating how to read the proprietary data file
2. Extracting the metadata
3. Extracting the data
4. Writing the metadata and data to a h5USID file

The goal of this document is to demonstrate how one would extract data and parameters from a Scanning Tunnelling
Spectroscopy (STS) raw data file obtained from an Omicron Scanning Tunneling Microscope (STM) into a h5USID file.

While there is an
`AscTranslator <https://github.com/pycroscopy/pycroscopy/blob/master/pycroscopy/io/translators/omicron_asc.py>`_
available in `pycroscopy` that can translate these files in just a **single** line,
we will intentionally assume that no such translator is available. Using a handful of useful functions in pyUSID,
we will translate the files from the source **.asc** format to h5USID files.


Recommended pre-requisite reading
---------------------------------

Before proceeding with this example, we recommend reading the previous documents to learn more about:

* `Universal Spectroscopic and Imaging Data (USID) model </../../USID/usid_model.html>`_
* The `previous chapter <./plot_numpy_translator.html>`_ for this topic that goes over the NumpyTranslator in greater
  detail

.. tip::
    You can download and run this document as a Jupyter notebook using the link at the bottom of this page.

Import all necessary packages
-----------------------------
There are a few setup procedures that need to be followed before any code is written. In this step, we simply load a
few python packages that will be necessary in the later steps.
"""

# Ensure python 3 compatibility:
from __future__ import division, print_function, absolute_import, unicode_literals

# The package for accessing files in directories, etc.:
import os
import zipfile

# Warning package in case something goes wrong
from warnings import warn
import subprocess
import sys


def install(package):
    subprocess.call([sys.executable, "-m", "pip", "install", package])
# Package for downloading online files:
try:
    # This package is not part of anaconda and may need to be installed.
    import wget
except ImportError:
    warn('wget not found.  Will install with pip.')
    import pip
    install(wget)
    import wget

# The mathematical computation package:
import numpy as np

# The package used for creating and manipulating HDF5 files:
import h5py

# Packages for plotting:
import matplotlib.pyplot as plt

# Finally import pyUSID:
try:
    import pyUSID as usid
except ImportError:
    warn('pyUSID not found.  Will install with pip.')
    import pip
    install('pyUSID')
    import pyUSID as usid

####################################################################################
# Step 0. Procure the Raw Data file
# =================================
# Here we will download a compressed data file from Github and unpack it:

url = 'https://raw.githubusercontent.com/pycroscopy/pycroscopy/master/data/STS.zip'
zip_path = 'STS.zip'
if os.path.exists(zip_path):
    os.remove(zip_path)
_ = wget.download(url, zip_path, bar=None)

zip_path = os.path.abspath(zip_path)
# figure out the folder to unzip the zip file to
folder_path, _ = os.path.split(zip_path)
zip_ref = zipfile.ZipFile(zip_path, 'r')
# unzip the file
zip_ref.extractall(folder_path)
zip_ref.close()
# delete the zip file
os.remove(zip_path)

data_file_path = 'STS.asc'

####################################################################################
# Step 1. Explore the Raw Data File
# =================================
#
# Inherently, one may not know how to read these ``.asc`` files. One option is to try and read the file as a text file
# one line at a time.
#
# If one is lucky, as in the case of these ``.asc`` files, the file can be read like conventional text files.
#
# Here is how we tested to see if the ``asc`` files could be interpreted as text files. Below, we read just the first 10
# lines in the file

with open(data_file_path, 'r') as file_handle:
    for lin_ind in range(10):
        print(file_handle.readline().replace('\n', ''))

####################################################################################
# Step 2. Read the contents of the file
# =====================================
# Now that we know that these files are simple text files, we can manually go through the file to find out which lines
# are important, at what lines the data starts etc.
# Manual investigation of such ``.asc`` files revealed that these files are always formatted in the same way. Also, they
# contain parameters in the first ``403`` lines and then contain data which is arranged as one pixel per row.
# STS experiments result in 3 dimensional datasets ``(X, Y, current)``. In other words, a 1D array of current data (as a
# function of excitation bias) is sampled at every location on a two dimensional grid of points on the sample.
# By knowing where the parameters are located and how the data is structured, it is possible to extract the necessary
# information from these files.
# Since we know that the data sizes (<200 MB) are much smaller than the physical memory of most computers, we can start
# by safely loading the contents of the entire file to memory.

# Reading the entire file into memory
with open(data_file_path, 'r') as file_handle:
    string_lines = file_handle.readlines()

####################################################################################
# Step 3. Read the parameters
# ===========================
# The parameters in these files are present in the first few lines of the file

# Preparing an empty dictionary to store the metadata / parameters as key-value pairs
parm_dict = dict()

# Reading parameters stored in the first few rows of the file
for line in string_lines[3:17]:
    # Remove the hash / pound symbol, if any
    line = line.replace('# ', '')
    # Remove new-line escape-character, if any
    line = line.replace('\n', '')
    # Break the line into two parts - the parameter name and the corresponding value
    temp = line.split('=')
    # Remove spaces in the value. Remember, the value is still a string and not a number
    test = temp[1].strip()
    # Now, attempt to convert the value to a number (floating point):
    try:
        test = float(test)
        # In certain cases, the number is actually an integer, check and convert if it is:
        if test % 1 == 0:
            test = int(test)
    except ValueError:
        pass
    parm_dict[temp[0].strip()] = test

# Print out the parameters extracted
for key in parm_dict.keys():
    print(key, ':\t', parm_dict[key])

####################################################################################
# At this point, we recommend reformatting the parameter names to standardized nomenclature.
# We realize that the materials imaging community has not yet agreed upon standardized nomenclature for metadata.
# Therefore, we leave this as an optional, yet recommended step.
# For example, in pycroscopy, we may categorize the number of rows and columns in an image under ``grid`` and
# data sampling parameters under ``IO``.
# As an example, we may rename ``x-pixels`` to ``positions_num_cols`` and ``y-pixels`` to ``positions_num_rows``.
#
# Step 3.a Prepare to read the data
# ==================================
# Before we read the data, we need to make an empty array to store all this data. In order to do this, we need to read
# the dictionary of parameters we made in step 2 and extract necessary quantities

num_rows = int(parm_dict['y-pixels'])
num_cols = int(parm_dict['x-pixels'])
num_pos = num_rows * num_cols
spectra_length = int(parm_dict['z-points'])

####################################################################################
# Step 3.b Read the data
# ======================
# We have observed that the data in these ``.asc`` files are consistently present after the first ``403`` lines of
# parameters. Using this knowledge, we need to populate a data array using data that is currently present as text lines
# in memory (from step 2).
# Recall that in step 2, we were lucky enough to read the entire data file into memory given its small size.
# The data is already present in memory as a list of strings that need to be parsed as a matrix of numbers.

# num_headers = len(string_lines) - num_pos
num_headers = 403

# These ``.asc`` file store the 3D data (X, Y, spectra) as a 2D matrix (positions, spectra). In other words, the spectra
# are arranged one below another. Thus, reading the 2D matrix from top to bottom, the data arranged column-by-column,
# and then row-by-row So, for simplicity, we will prepare an empty 2D numpy array to store the data as it exists in the
# raw data file:
raw_data_2d = np.zeros(shape=(num_pos, spectra_length), dtype=np.float32)

# Iterate over ever measurement position:
for pos_index in range(num_pos):
    # First, get the correct (string) line corresponding to the current measurement position.
    # Recall that we would need to skip the many header lines to get to the data
    this_line = string_lines[num_headers + pos_index]
    # Each (string) line contains numbers separated by tabs (``\t``). Let us break the line into several shorter strings
    # each containing one number. We will ignore the last entry since it is empty.
    string_spectrum = this_line.split('\t')[:-1]  # omitting the new line
    # Now that we have a list of numbers represented as strings, we need to convert this list to a 1D numpy array
    # the converted array is set to the appropriate position in the main 2D array.
    raw_data_2d[pos_index] = np.array(string_spectrum, dtype=np.float32)

####################################################################################
# If the data is so large that it cannot fit into memory, we would need to read data one (or a few) position at a time,
# process it (e.g. convert from string to numbers), and write it to the HDF5 file without keeping much or any data
# in memory.

# Step 4.a Describing the collected data itself
# =============================================
# So far, we managed to read the parameters into a dictionary and the data itself into a 2D numpy array.
# We do not have ancillary information such as the physical quantity and units of the data, the values of the bias
# against which the spectra were acquired, etc. We will begin formally specifying each of these components:

sci_data_type = 'STS'
quantity = 'Current'
units = 'nA'

####################################################################################
# Step 4.b Preparing ancillary information
# ========================================
# pyUSID makes it easy to specify the reference position and spectroscopic axes and scalebars against which data was
# acquired through the ``Dimension`` class. In this example, we have two position dimensions - ``X`` and ``Y`` and one
# spectroscopy dimension - ``Bias`` against which data for each spectra were collected. Each dimension is specified
# using three pieces of information - the name, physical units, and either the number of steps (if starting from 0 and
# having a step size of 1) or the waveform that expresses how the dimension was varied.
#
# Note that the position and spectroscopic dimensions need to be arranged from fastest-varying to slowest-varying.
#
# The spectroscopic dimensions are trivial since we only have one dimension.

# The correct value of this parameter was not clear in the metadata. We will use 1 V for illustrative purposes.
max_v = 1

# Generate the voltage / spectroscopic axis:
volt_vec = np.linspace(-1 * max_v, 1 * max_v, spectra_length)

# Prepare the Dimension object:
spec_dims = usid.write_utils.Dimension('Bias', 'V', volt_vec)

####################################################################################
# Given that the spectra were acquired column-by-column and then row-by-row, we would need to arrange them as ``X``
# followed by ``Y``.
#
# Note that the correct values over which these dimensions were varied was not clear in the
# metadata. For illustrative purposes we will assume that the X and Y positions started from 0 and increased by 1
# at each step.

pos_dims = [usid.write_utils.Dimension('X', 'a. u.', parm_dict['x-pixels']),
            usid.write_utils.Dimension('Y', 'a. u.', parm_dict['y-pixels'])]

####################################################################################
# Step 4.c Preparing the name of the new HDF5 file
# ================================================
# Below, we will specify the name of the HDF5 file that we want to write the prepared data and metadata:

# First, let us separate the file name from the path to the folder containing the raw data file
folder_path, file_name = os.path.split(data_file_path)
# Next, we will remove the ``.asc`` extension
file_name = file_name[:-4] + '_'
# The new file name will share the same base name as the original file but will end with a ``.h5`` extension.
# This HDF5 or H5 file will live in the same folder as the raw data file
h5_path = os.path.join(folder_path, file_name + '.h5')

####################################################################################
# Indeed, a simple ``replace('.asc', '.h5')`` might have done the same job. However, the above method is recommended
#
# Step 4.d Writing to a h5USID file
# =================================
# There are multiple methods for writing to a h5USID file:
#
# a. Using high-level tools such as the ``NumpyTranslator`` in pyUSID - done in two lines!
# b. Using low-level tools in ``pyUSID.hdf_utils`` - slightly longer but full flexibility
# c. Using basic functions from the ``h5py`` package (that pyUSID already uses) - long and laborious
#
# We will first illustrate how easy it is to write a USID compliant HDF5 file for this data using the `NumpyTranslator`:

tran = usid.NumpyTranslator()
h5_path = tran.translate(h5_path, sci_data_type, raw_data_2d,  quantity, units,
                         pos_dims, spec_dims, translator_name='Omicron_ASC_Translator', parm_dict=parm_dict)

####################################################################################
# The NumpyTranslator simplifies the creation of h5USID files. It creates the HDF5 file, creates HDF5 datasets and
# groups, creates ancillary HDF5 datasets, writes parameters, links ancillary datasets to the main dataset etc.
# With a single call to the NumpyTranslator, we complete the translation process.
#
# Verifying the newly written H5 file:
# ====================================
# Let us perform some simple and quick verification to show that the data has indeed been translated correctly:

with h5py.File(h5_path, mode='r') as h5_file:
    # See if a tree has been created within the hdf5 file:
    usid.hdf_utils.print_tree(h5_file)
    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
    h5_main = usid.hdf_utils.get_all_main(h5_file)[-1]
    print(h5_main)
    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
    fig, axes = plt.subplots(ncols=2, figsize=(11, 5))
    spat_map = np.reshape(h5_main[:, 100], (100, 100))
    usid.plot_utils.plot_map(axes[0], spat_map, origin='lower')
    axes[0].set_title('Spatial map')
    axes[0].set_xlabel('X')
    axes[0].set_ylabel('Y')
    axes[1].plot(np.linspace(-1.0, 1.0, h5_main.shape[1]),
                 h5_main[250])
    axes[1].set_title('IV curve at a single pixel')
    axes[1].set_xlabel('Tip bias [V]')
    axes[1].set_ylabel('Current [nA]')

    fig.tight_layout()


####################################################################################
# Notes on translation
# =====================
# * Steps 1-3 would be performed anyway in order to begin data analysis
# * The actual procedure for translation to h5USID is reduced to just 3-4 lines in step 4.
# * A modular / formal version of this translator has been implemented as a class in pycroscopy as the
#   `AscTranslator <https://github.com/pycroscopy/pycroscopy/blob/master/pycroscopy/io/translators/omicron_asc.py>`_.
#   This custom translator packages the same code used above into functions that focus on the individual tasks such
#   as extracting parameters, reading data, and writing to h5USID. The ``NumpyTranslator`` uses the
#   ``pyUSID.hdf_utils.write_main_dataset()`` function underneath to write its data. You can learn more about lower-
#   level file-writing functions in another tutorial on `writing <./plot_hdf_utils_write.html>`_ h5USID files.
# * There are many benefits to writing such a formal Translator class instead of standalone scripts like this including:
#
#   * Unlike such a stand-alone script, a Translator class in the package can be used by everyone repeatedly
#   * The custom Translator class can ensure consistency when translating multiple files. 
#   * A single, robust Translator class can handle the finer variations / modes in the data. See the
#     `IgorIBWTranslator <https://github.com/pycroscopy/pycroscopy/blob/master/pycroscopy/io/translators/igor_ibw.py>`_
#     as an example.
#
# * While this approach is feasible and encouraged for simple and small data, it may be necessary to use lower level
#   calls to write efficient translators. As an example, please see the `BEPSndfTranslator
#   <https://github.com/pycroscopy/pycroscopy/blob/master/pycroscopy/io/translators/beps_ndf.py>`_
# * We have found python packages online to open a few proprietary file formats and have written translators using these
#   packages. If you are having trouble reading the data in your files and cannot find any packages online, consider 
#   contacting the manufacturer of the instrument which generated the data in the proprietary format for help.
#

# Remove both the original and translated files:
os.remove(h5_path)
os.remove(data_file_path)
